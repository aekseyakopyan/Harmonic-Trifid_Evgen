import pybreaker
import httpx
from typing import Optional, Dict, Any
from core.config.settings import settings
from core.utils.logger import logger

# Ð˜ÑÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Circuit Breaker
class LLMAPIError(Exception):
    pass

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Circuit Breaker
api_breaker = pybreaker.CircuitBreaker(
    fail_max=5,              # ÐŸÐ¾ÑÐ»Ðµ 5 Ð¾ÑˆÐ¸Ð±Ð¾Ðº â†’ OPEN
    timeout_duration=60,     # Recovery Ñ‡ÐµÑ€ÐµÐ· 60 ÑÐµÐºÑƒÐ½Ð´
    expected_exception=LLMAPIError,
    name="llm_circuit_breaker"
)

class ResilientLLMClient:
    """
    ÐšÐ»Ð¸ÐµÐ½Ñ‚ Ð´Ð»Ñ Ð˜Ð˜-Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹ Circuit Breaker Ð¸ Fallback-Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸.
    CLOSED â†’ OPEN â†’ HALF_OPEN
    """
    
    def __init__(self):
        self.api_key = settings.OPENROUTER_API_KEY
        self.model = settings.OPENROUTER_MODEL or "deepseek/deepseek-chat"
        self.base_url = "https://openrouter.ai/api/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://harmonic-trifid.local",
            "X-Title": "Harmonic Trifid Resilient"
        }

    @api_breaker
    async def _call_openrouter(self, model: str, prompt: str, system_prompt: str) -> str:
        """ÐœÐµÑ‚Ð¾Ð´ Ð¾Ð±ÐµÑ€Ð½ÑƒÑ‚Ñ‹Ð¹ Ð² Circuit Breaker Ð´Ð»Ñ Ð²Ñ‹Ð·Ð¾Ð²Ð° Ð²Ð½ÐµÑˆÐ½ÐµÐ³Ð¾ API."""
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.3,
            "max_tokens": 4096
        }
        
        try:
            async with httpx.AsyncClient(timeout=10.0) as client: # Ð£Ð¼ÐµÐ½ÑŒÑˆÐ°ÐµÐ¼ Ñ‚Ð°Ð¹Ð¼Ð°ÑƒÑ‚ Ð´Ð¾ 10Ñ Ð´Ð»Ñ fail-fast
                response = await client.post(self.base_url, headers=self.headers, json=payload)
                
                if response.status_code == 429:
                    raise LLMAPIError("Rate limit exceeded")
                if response.status_code >= 500:
                    raise LLMAPIError(f"Server error: {response.status_code}")
                
                response.raise_for_status()
                data = response.json()
                
                if "choices" in data and len(data["choices"]) > 0:
                    return data["choices"][0]["message"]["content"]
                
                raise LLMAPIError("Empty response from provider")
        except (httpx.RequestError, httpx.HTTPStatusError) as e:
            raise LLMAPIError(f"Network error: {str(e)}")

    async def generate_response(self, prompt: str, system_prompt: str = "You are a helpful assistant.") -> Optional[str]:
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ Ð¼Ð½Ð¾Ð³Ð¾ÑƒÑ€Ð¾Ð²Ð½ÐµÐ²Ñ‹Ð¼ Fallback."""
        
        # 1. ÐŸÐ¾Ð¿Ñ‹Ñ‚ÐºÐ° Ñ‡ÐµÑ€ÐµÐ· OpenRouter (Ñ Circuit Breaker)
        try:
            return await self._call_openrouter(self.model, prompt, system_prompt)
        except pybreaker.CircuitBreakerError:
            logger.warning("ðŸš¨ [CB] OpenRouter Circuit is OPEN. Failing fast to fallback.")
        except Exception as e:
            logger.error(f"âŒ [CB] OpenRouter call failed: {e}")

        # 2. Fallback Ð½Ð° Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½ÑƒÑŽ Ollama (ÐµÑÐ»Ð¸ OpenRouter Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½)
        logger.info("ðŸ”„ Switching to local Ollama fallback...")
        return await self._generate_ollama(settings.OLLAMA_MODEL, prompt, system_prompt)

    async def _generate_ollama(self, model: str, prompt: str, system_prompt: str) -> Optional[str]:
        """Ð›Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ fallback-Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼."""
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            "stream": False
        }
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(settings.OLLAMA_URL, json=payload)
                response.raise_for_status()
                data = response.json()
                return data.get("message", {}).get("content", "")
        except Exception as e:
            logger.error(f"ðŸ”¥ Critical: Local Ollama fallback also failed: {e}")
            return None

# Singleton
resilient_llm = ResilientLLMClient()
