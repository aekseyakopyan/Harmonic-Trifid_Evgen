import asyncio
import sqlite3
import os
import sys
import json
import re

# Add project root to path
sys.path.append(os.getcwd())

from core.ai_engine.llm_client import LLMClient
from core.utils.logger import logger

# System prompt for batch classification
SYSTEM_PROMPT = """
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–æ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥—É. –¢–≤–æ—è –∑–∞–¥–∞—á–∞: –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ Telegram.

–ö–ê–¢–ï–ì–û–†–ò–ò:
1. BUYER ‚Äî –∫—Ç–æ-—Ç–æ –∏—â–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞, –ø–æ–¥—Ä—è–¥—á–∏–∫–∞, –∑–∞–¥–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –ø–æ —É—Å–ª—É–≥–µ, –ø—Ä–æ—Å–∏—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.
2. SELLER ‚Äî –∫—Ç–æ-—Ç–æ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å–≤–æ–∏ —É—Å–ª—É–≥–∏, –∫–µ–π—Å—ã, –æ–±—É—á–µ–Ω–∏–µ, –ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç –≤ –∫–∞–Ω–∞–ª, —Å–ø–∞–º –æ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤.
3. IRRELEVANT ‚Äî –º—É—Å–æ—Ä, –Ω–æ–≤–æ—Å—Ç–∏, –æ–±—â–µ–Ω–∏–µ –Ω–∏ –æ —á–µ–º.

–§–æ—Ä–º–∞—Ç –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: JSON —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ {"id": number, "text": string}.
–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê: –¢–æ–ª—å–∫–æ JSON —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ {"id": number, "result": "BUYER" | "SELLER" | "IRRELEVANT"}.
–ù–∏–∫–∞–∫–æ–≥–æ –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –æ—Ç–≤–µ—Ç–µ, —Ç–æ–ª—å–∫–æ —á–∏—Å—Ç—ã–π JSON.
"""

BATCH_SIZE = 120

async def filter_leads_batch(limit=None):
    db_path = "data/db/all_historical_leads.db"
    if not os.path.exists(db_path):
        print(f"‚ùå –ë–∞–∑–∞ {db_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        return

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # Migration: Add columns if not exist
    try:
        cursor.execute("ALTER TABLE all_historical_leads ADD COLUMN llm_reason TEXT;")
        cursor.execute("ALTER TABLE all_historical_leads ADD COLUMN llm_marker TEXT;")
    except sqlite3.OperationalError:
        pass # Already exists
    
    # Load only non-processed leads
    cursor.execute("SELECT id, text FROM all_historical_leads WHERE llm_status IS NULL LIMIT 2000") # Process in chunks
    
    leads = cursor.fetchall()
    
    if not leads:
        print("‚úÖ –ù–µ—Ç –Ω–æ–≤—ã—Ö –ª–∏–¥–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.")
        return

    print(f"üöÄ –ù–∞—á–∏–Ω–∞—é –ø–∞–∫–µ—Ç–Ω—É—é LLM-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é {len(leads)} –ª–∏–¥–æ–≤ —á–µ—Ä–µ–∑ DeepSeek (–ø–∞–∫–µ—Ç—ã –ø–æ {BATCH_SIZE})...")
    llm_client = LLMClient()
    
    processed_this_session = 0
    total_leads = 4466 # approximate
    
    # Process in batches
    for i in range(0, len(leads), BATCH_SIZE):
        batch = leads[i:i+BATCH_SIZE]
        batch_data = [{"id": l[0], "text": l[1][:400].replace("\n", " ")} for l in batch]
        
        try:
            prompt = f"–†–∞–∑–æ–±—Ä–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è:\n{json.dumps(batch_data, ensure_ascii=False)}"
            response = await llm_client.generate_response(prompt=prompt, system_prompt=SYSTEM_PROMPT)
            
            if not response:
                print(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –ò–ò –¥–ª—è –ø–∞–∫–µ—Ç–∞ {i//BATCH_SIZE + 1}")
                continue

            # Heavy-duty JSON extraction
            json_str = response.strip()
            # Remove Markdown code blocks
            json_str = re.sub(r'```json\s*|\s*```', '', json_str)
            json_str = re.sub(r'```\s*|\s*```', '', json_str)
            
            # Find the list start and end
            start_idx = json_str.find('[')
            end_idx = json_str.rfind(']')
            
            if start_idx != -1 and end_idx != -1:
                json_str = json_str[start_idx:end_idx+1]
            
            try:
                results = json.loads(json_str)
            except json.JSONDecodeError as je:
                # If JSON fails, try a desperate regex approach for each item
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON, –ø—ã—Ç–∞—é—Å—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ Regex...")
                results = []
                # Handle cases like {id: 123, result: BUYER, reason: ..., marker: ...}
                # Reason and marker might be omitted by LLM if it gets lazy
                matches = re.finditer(r'\{[^{}]*"?id"?:\s*(\d+)[^{}]*"?result"?:\s*"?(\w+)"?(?:[^{}]*"?reason"?:\s*"?(.*?)"?)?(?:[^{}]*"?marker"?:\s*"?(.*?)"?)?[^{}]*\}', json_str, re.IGNORECASE)
                for m in matches:
                    results.append({
                        "id": int(m.group(1)), 
                        "result": m.group(2).upper(),
                        "reason": m.group(3) if m.lastindex >= 3 and m.group(3) else "",
                        "marker": m.group(4) if m.lastindex >= 4 and m.group(4) else ""
                    })
                
                if not results:
                    print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Å—Ç–∏ –ø–∞–∫–µ—Ç {i//BATCH_SIZE + 1}. –û—Ç–≤–µ—Ç –±—ã–ª: {response[:300]}...")
                    raise je

            ids_to_delete = []
            ids_to_mark_buyer = []
            updates = []

            for res in results:
                r_id = res.get("id")
                status = str(res.get("result", "")).upper()
                reason = res.get("reason", "")
                marker = res.get("marker", "")
                
                if "SELLER" in status or "IRRELEVANT" in status:
                    ids_to_delete.append(r_id)
                elif "BUYER" in status:
                    ids_to_mark_buyer.append(r_id)
                
                updates.append((status, reason, marker, r_id))

            # Update DB with reasoning
            for update in updates:
                cursor.execute("UPDATE all_historical_leads SET llm_status=?, llm_reason=?, llm_marker=? WHERE id=?", update)

            # Log deleted for analysis
            if ids_to_delete:
                with open("logs/deleted_leads_for_analysis.log", "a", encoding="utf-8") as df:
                    for d_id in ids_to_delete:
                        # Find the text, reason, marker
                        for res in results:
                            if res.get("id") == d_id:
                                # Find text in batch
                                text = next((l[1] for l in batch if l[0] == d_id), "N/A")
                                df.write(f"--- ID: {d_id} | Reason: {res.get('reason')} | Marker: {res.get('marker')} ---\n{text}\n")
                
                cursor.execute(f"DELETE FROM all_historical_leads WHERE id IN ({','.join(map(str, ids_to_delete))})")
            
            conn.commit()

            processed_this_session += len(batch)
            progress = (processed_this_session / len(leads)) * 100
            print(f"[{progress:.2f}%] | –ü–∞–∫–µ—Ç {i//BATCH_SIZE + 1} –æ–±—Ä–∞–±–æ—Ç–∞–Ω (–£–¥–∞–ª–µ–Ω–æ: {len(ids_to_delete)}, –û—Å—Ç–∞–≤–ª–µ–Ω–æ: {len(ids_to_mark_buyer)})")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ –ø–∞–∫–µ—Ç–µ {i//BATCH_SIZE + 1}: {e}")
            await asyncio.sleep(5)
            
    print(f"üèÅ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_this_session} –∑–∞–ø–∏—Å–µ–π.")
    conn.close()

if __name__ == "__main__":
    asyncio.run(filter_leads_batch())
