import os
import asyncio
from datetime import datetime
from telethon import TelegramClient, events
from telethon.tl.custom import Message
from telethon.events import NewMessage, MessageRead, ChatAction
from systems.gwen import create_interceptor
from core.database.connection import async_session
from core.database.models import Lead, MessageLog
from sqlalchemy import select, update, and_
from core.classifier.intent_classifier import MessageClassifier
from core.ai_engine.llm_client import llm_client
from core.ai_engine.prompt_builder import prompt_builder
from core.knowledge_base.retriever import KnowledgeRetriever
from core.utils.logger import logger
from core.config.settings import settings
from core.utils.handover import handover_manager
from systems.gwen.gwen_supervisor import gwen_supervisor

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π
message_buffers = {}
# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
user_data_cache = {}
debounce_tasks = {}

async def handle_user_action(client: TelegramClient, event: events.ChatAction.Event):
    """
    Handler for user actions like 'typing' or 'recording'.
    Resets the debounce timer to wait for completion.
    """
    sender_id = event.user_id
    if not sender_id:
        return

    # –ù–∞–º –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–æ –ø–µ—á–∞—Ç–∏ –∏–ª–∏ –∑–∞–ø–∏—Å–∏
    if event.typing or event.recording:
        # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –≤ –∫–µ—à–µ, –Ω–µ –∑–∞—Ç–∏—Ä–∞—è –¥—Ä—É–≥–∏–µ –¥–∞–Ω–Ω—ã–µ
        if sender_id not in user_data_cache:
            user_data_cache[sender_id] = {}
            
        user_data_cache[sender_id]['last_action_at'] = datetime.utcnow()
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º event —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ–≥–æ —Ç–∞–º –Ω–µ—Ç (—á—Ç–æ–±—ã –Ω–µ –∑–∞—Ç–µ—Ä–µ—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π event —Å–æ–æ–±—â–µ–Ω–∏—è)
        if 'event' not in user_data_cache[sender_id]:
            user_data_cache[sender_id]['event'] = event
            
        logger.debug(f"User {sender_id} is active ({'typing' if event.typing else 'recording'}). Resetting timer.")

async def handle_incoming_message(event, client: TelegramClient):
    """
    Handler for incoming messages with debouncing.
    Wait for more messages or actions from the same user.
    """
    message = event.message if hasattr(event, 'message') else event
    sender = await event.get_sender()
    sender_id = sender.id if sender else 0
    
    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–≤–æ–∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
    me = await client.get_me()
    if sender_id == me.id:
        return

    # 0. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
    text = ""
    if message.voice:
        logger.info(f"Voice message from {sender_id}. Downloading...")
        os.makedirs("downloads", exist_ok=True)
        file_path = await message.download_media(file="downloads/")
        from core.audio.transcriber import transcriber
        text = await transcriber.transcribe(file_path)
        if os.path.exists(file_path):
            os.remove(file_path)
    else:
        text = message.text

    if not text:
        return

    # 1. –î–æ–±–∞–≤–ª—è–µ–º –≤ –±—É—Ñ–µ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    if sender_id not in message_buffers:
        message_buffers[sender_id] = []
    message_buffers[sender_id].append(text)

    # 2. –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
    if sender_id not in user_data_cache:
        user_data_cache[sender_id] = {}
        
    user_data_cache[sender_id].update({
        'last_action_at': datetime.utcnow(),
        'sender': sender,
        'event': event
    })

    # 3. –£–ø—Ä–∞–≤–ª—è–µ–º —Ç–∞–π–º–µ—Ä–æ–º (debounce)
    if sender_id in debounce_tasks:
        debounce_tasks[sender_id].cancel()

    # –ó–∞–ø—É—Å–∫–∞–µ–º –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –æ–∂–∏–¥–∞–Ω–∏—è –Ω–∞ 10 —Å–µ–∫—É–Ω–¥
    task = asyncio.create_task(wait_and_process(client, sender_id))
    debounce_tasks[sender_id] = task

async def wait_and_process(client, sender_id):
    """–û–∂–∏–¥–∞–µ—Ç —Ç–∏—à–∏–Ω—ã –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π. –¢–∞–π–º–µ—Ä –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π."""
    try:
        # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è (–±—ã—Å—Ç—Ä–µ–µ –≤ –∞–∫—Ç–∏–≤–Ω–æ–º –¥–∏–∞–ª–æ–≥–µ)
        wait_time = 5.0 # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        async with async_session() as session:
            stmt = select(Lead).where(Lead.telegram_id == sender_id)
            res = await session.execute(stmt)
            lead = res.scalars().first()
            if lead and lead.last_interaction:
                # –ï—Å–ª–∏ –¥–∏–∞–ª–æ–≥ –∞–∫—Ç–∏–≤–µ–Ω (–ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ < 5 –º–∏–Ω—É—Ç –Ω–∞–∑–∞–¥)
                diff = (datetime.utcnow() - lead.last_interaction).total_seconds()
                if diff < 300:
                    wait_time = 2.0 # –í –∞–∫—Ç–∏–≤–Ω–æ–º —á–∞—Ç–µ –∂–¥–∞—Ç—å 2 —Å–µ–∫ –≤–ø–æ–ª–Ω–µ –æ–∫
                else:
                    wait_time = 5.0
            else:
                wait_time = 7.0 # –ü—Ä–∏ –ø–µ—Ä–≤–æ–º –∫–æ–Ω—Ç–∞–∫—Ç–µ –∂–¥–µ–º –ø–æ–¥–æ–ª—å—à–µ (–≤–¥—Ä—É–≥ –ø–∏—à–µ—Ç –ø—Ä–æ—Å—Ç—ã–Ω—é)
        
        while True:
            await asyncio.sleep(1) # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é —Å–µ–∫—É–Ω–¥—É
            
            last_activity = user_data_cache.get(sender_id, {}).get('last_action_at')
            if not last_activity:
                break
                
            elapsed = (datetime.utcnow() - last_activity).total_seconds()
            if elapsed >= wait_time:
                # –¢–∏—à–∏–Ω–∞ –¥–ª–∏–ª–∞—Å—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–æ–ª–≥–æ ‚Äî –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
                break
            else:
                # –ï—â–µ –Ω–µ—Ç 10 —Å–µ–∫—É–Ω–¥ —Ç–∏—à–∏–Ω—ã ‚Äî –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∂–¥–∞—Ç—å
                continue
        
        # –ï—Å–ª–∏ –¥–æ–∂–¥–∞–ª–∏—Å—å ‚Äî –∏–∑–≤–ª–µ–∫–∞–µ–º –≤—Å—ë –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–µ
        texts = message_buffers.pop(sender_id, [])
        data = user_data_cache.pop(sender_id, {})
        
        if not texts:
            return
            
        full_text = "\n".join(texts)
        sender = data.get('sender')
        event = data.get('event')
        
        logger.info(f"Processing thought from {sender.first_name if sender else 'Unknown'} after 10s silence.")
        await process_full_thought(client, event, sender, full_text)
        
    except asyncio.CancelledError:
        pass
    except Exception as e:
        logger.error(f"Error in wait_and_process: {e}")
        # –£–≤–µ–¥–æ–º–∏—Ç—å –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞ –æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–µ
        try:
            from core.utils.admin_notifier import AdminNotifier
            notifier = AdminNotifier(client)
            await notifier.notify_error(
                e,
                "–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è",
                {
                    "name": "Unknown",
                    "id": sender_id
                }
            )
        except:
            pass  # –ù–µ –ø–∞–¥–∞–µ–º, –µ—Å–ª–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –Ω–µ –æ—Ç–ø—Ä–∞–≤–∏–ª–æ—Å—å
    finally:
        if debounce_tasks.get(sender_id) == asyncio.current_task():
            debounce_tasks.pop(sender_id, None)

async def process_full_thought(client: TelegramClient, event: NewMessage.Event, sender, full_text: str):
    """Core logic to process the combined message."""
    sender_id = sender.id if sender else 0
    sender_name = getattr(sender, 'first_name', '') or getattr(sender, 'title', 'Unknown')
    sender_username = getattr(sender, 'username', None)

    async with async_session() as session:
        # 1. Get or create lead
        stmt = select(Lead).where(Lead.telegram_id == sender_id)
        result = await session.execute(stmt)
        lead = result.scalars().first()
        
        if not lead:
            lead = Lead(
                telegram_id=sender_id,
                username=sender_username,
                full_name=sender_name
            )
            session.add(lead)
            await session.commit()
            await session.refresh(lead)
        
        # Guard: –ï—Å–ª–∏ –¥–∏–∞–ª–æ–≥ –ø–µ—Ä–µ–¥–∞–Ω —á–µ–ª–æ–≤–µ–∫—É ‚Äî –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –≤—Ö–æ–¥—è—â–∏–µ –¥–ª—è –ò–ò
        if lead.is_human_managed:
            logger.info(f"‚è∏ Skipping AI processing for Lead {sender_id} (Human Managed)")
            return
            
        lead.follow_up_level = 0
        lead.follow_up_sent_at = None

        # 1.1 History
        from sqlalchemy import desc
        history_stmt = select(MessageLog).where(MessageLog.lead_id == lead.id).order_by(desc(MessageLog.created_at)).limit(10)
        history_result = await session.execute(history_stmt)
        history_msgs = history_result.scalars().all()
        history_msgs.reverse()
        
        history_items = []
        for m in history_msgs:
            role = "–¢–´ (–ê–ª–µ–∫—Å–µ–π)" if m.direction == "outgoing" else "–ö–õ–ò–ï–ù–¢"
            history_items.append(f"{role}: {m.content}")
        history_text = "\n".join(history_items)

        # 2. Classify
        classifier = MessageClassifier()
        classification = await classifier.classify(full_text)
        
        # 3. Context
        retriever = KnowledgeRetriever(session)
        cases = await retriever.find_relevant_cases(full_text)
        service = await retriever.find_service_by_category(classification.get("category", ""))
        
        # 3.1 KB Search (B2B Sales Materials)
        sales_materials = await retriever.search_markdown_kb(full_text)

        # 3.2 Web Search if needed
        external_cases = []
        if len(cases) < 2 and classification.get("category") not in ["general", None]:
            from core.knowledge_base.web_searcher import web_searcher
            niche = full_text # Simplification: use full_text as niche context
            service_name = service.name if service else classification.get("category")
            external_cases = await web_searcher.search_cases(niche, service_name)

        # 4. Prompt
        # 4.1 Dynamic Emotion Selection based on tone and history
        tone = classification.get("tone", "neutral")
        if "hurry" in tone:
            current_emotion = "interested" # –ï—Å–ª–∏ –∫–ª–∏–µ–Ω—Ç —Ç–æ—Ä–æ–ø–∏—Ç—Å—è, –º—ã –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–æ–≤–ª–µ—á–µ–Ω—ã
        elif tone == "negative":
            current_emotion = "skeptical" # –ù–∞ –Ω–µ–≥–∞—Ç–∏–≤ –æ—Ç–≤–µ—á–∞–µ–º —Å–∫–µ–ø—Å–∏—Å–æ–º
        elif tone == "positive":
            current_emotion = "interested" # –ù–∞ –ø–æ–∑–∏—Ç–∏–≤ ‚Äî –∏–Ω—Ç–µ—Ä–µ—Å–æ–º
        elif len(history_msgs) > 5:
            current_emotion = "interested" # –ü–æ—Å–ª–µ 5 —Å–æ–æ–±—â–µ–Ω–∏–π –º—ã —É–∂–µ –ø–∞—Ä—Ç–Ω–µ—Ä—ã
        else:
            current_emotion = "skeptical" # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî –ª–µ–≥–∫–∏–π —Å–∫–µ–ø—Å–∏—Å —ç–∫—Å–ø–µ—Ä—Ç–∞
            
        task_instr = "–¢—ã ‚Äî –ê–ª–µ–∫—Å–µ–π, –æ—Ç–≤–µ—á–∞–µ—à—å –∫–ª–∏–µ–Ω—Ç—É –≤ –ª–∏—á–Ω–æ–π –ø–µ—Ä–µ–ø–∏—Å–∫–µ. –î–∞–π –∂–∏–≤–æ–π, —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏—Å—Ç–æ—Ä–∏–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."
        system_prompt = prompt_builder.build_system_prompt(task_instr)
        user_prompt = prompt_builder.build_user_prompt(
            query=full_text,
            user_name=sender_name,
            cases=cases,
            external_cases=external_cases,
            service=service,
            category=classification.get("category"),
            style_profile=lead.style_profile,
            context_memory=lead.context_memory,
            history_text=history_text,
            current_emotion=current_emotion,
            sales_materials=sales_materials
        )
        
        # 5. LLM Generation Loop with Supervisor
        MAX_RETRIES = 3
        current_system_prompt = system_prompt
        ai_response_text = ""
         
        for attempt in range(MAX_RETRIES):
            logger.info(f"üîÑ Generation attempt {attempt+1}/{MAX_RETRIES}...")
            ai_response_text = await llm_client.generate_response(user_prompt, current_system_prompt)
            
            if not ai_response_text:
                break # Handle as failure below
                
            # Check with Gwen
            check = await gwen_supervisor.check_message(ai_response_text)
            verdict = check.get("verdict", "ALLOW")
            
            if verdict == "ALLOW":
                logger.info("‚úÖ Gwen approved response.")
                break
                
            elif verdict == "BLOCK":
                logger.warning(f"‚õîÔ∏è Gwen HARD BLOCKED response: {check.get('reason')}")
                # Log failure and stop
                msg_log = MessageLog(
                    lead_id=lead.id, direction="outgoing", content=ai_response_text,
                    status="blocked", error_message=check.get("reason"),
                    intent=classification.get("intent") 
                )
                session.add(msg_log)
                await session.commit()
                
                # Notify Admin about Block
                try:
                   from systems.gwen.notifier import supervisor_notifier
                   await supervisor_notifier.notify_block(
                       f"{sender_name} (@{sender_username})", 
                       ai_response_text, 
                       check
                   )
                except: pass
                return # EXIT, DO NOT SEND
                
            elif verdict == "RETRY":
                logger.info(f"üîß Gwen requested RETRY: {check.get('reason')}")
                # Add correction instruction to prompt context for next attempt
                correction = check.get('correction', 'Improve quality.')
                current_system_prompt += f"\n\n[SUPERVISOR FEEDBACK]: The previous draft was rejected. Reason: {check.get('reason')}. instruction: {correction}. Fix this in the new response."
        else:
            # If loop finished without break (all retries failed)
            logger.warning("‚ö†Ô∏è Max retries reached. Sending best effort (or failing safely).")
            # Decision: Send the last attempt if it wasn't a hard block? 
            # Or fail? Let's fail safely for now to avoid bad reps.
            if verdict == "RETRY": 
                # If we are here, it means even the last attempt was a RETRY, 
                # but not a BLOCK. We might choose to send it anyway or silence.
                # Let's silence to be safe.
                 msg_log = MessageLog(
                    lead_id=lead.id, direction="outgoing", content=ai_response_text,
                    status="failed_quality", error_message="Max retries reached on Gwen check"
                )
                 session.add(msg_log)
                 await session.commit()
                 return

        
        if not ai_response_text:
            logger.error(f"Critical: LLM failed to generate response for user {sender_id}. Reporting to admin.")
            # –õ–æ–≥–∏—Ä—É–µ–º –≤ –ë–î –∫–∞–∫ –ø—Ä–æ–≤–∞–ª, –Ω–æ –ù–ï –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–ª–∏–µ–Ω—Ç—É
            msg_log = MessageLog(
                lead_id=lead.id, direction="incoming", content=full_text,
                intent=classification.get("intent"), category=classification.get("category"),
                lead_score=classification.get("lead_score"), ai_response=None,
                status="failed", error_message="All LLM models failed"
            )
            session.add(msg_log)
            await session.commit()
            return

        # 5.1 Detect if any case was mentioned OR if user asked for "screenshots/proofs"
        mentioned_cases = []
        is_requesting_proof = any(word in full_text.lower() for word in ["—Å–∫—Ä–∏–Ω", "–ø—Ä—É—Ñ", "—Ñ–æ—Ç–æ", "–ø–æ–∫–∞–∂–∏", "–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞", "–∫–∞–±–∏–Ω–µ—Ç"])
        
        if cases:
            for case in cases:
                if case.title.lower() in ai_response_text.lower() or (is_requesting_proof and case.category.lower() in full_text.lower()):
                    mentioned_cases.append(case)
        
        # 6. Log Incoming
        msg_log = MessageLog(
            lead_id=lead.id, direction="incoming", content=full_text,
            intent=classification.get("intent"), category=classification.get("category"),
            lead_score=classification.get("lead_score"), ai_response=ai_response_text,
            metadata_json=classification
        )
        session.add(msg_log)
        
        from core.utils.humanity import humanity_manager
        
        # 7. Humanity
        # 7.1 Reading Delay
        reading_delay = humanity_manager.get_reading_delay(full_text)
        logger.debug(f"Simulating reading delay: {reading_delay:.2f}s")
        await asyncio.sleep(reading_delay)
        
        # –ü–æ–º–µ—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –∫–∞–∫ –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω–æ–µ –ø–æ—Å–ª–µ "–ø—Ä–æ—á—Ç–µ–Ω–∏—è"
        try:
            await client.send_read_acknowledge(event.chat_id, event.message)
        except Exception as e:
            logger.warning(f"Failed to mark message as read: {e}")
            
        # 7.2 Prepare chunks
        chunks = humanity_manager.split_into_human_chunks(ai_response_text)
        
        sent_msg = None
        status = "sent"
        
        # –°–æ–∑–¥–∞–µ–º –ø–µ—Ä–µ—Ö–≤–∞—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏–π —Å —Å—É–ø–µ—Ä–≤–∏–∑–æ—Ä–æ–º –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        interceptor = client # Bypassing Gwen for now as requested or assumed for Alexey
        
        try:
            # 7.4 Handle Admin Escalation or Graceful Exit tag cleaning
            import re
            clean_response = ai_response_text
            
            # Case 1: Manual Help needed
            if "[ASK_ADMIN:" in ai_response_text:
                match = re.search(r"\[ASK_ADMIN:\s*(.*?)\]", ai_response_text)
                question = match.group(1) if match else "–í–æ–ø—Ä–æ—Å –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω"
                
                # Notify Admin
                admin_username = settings.ADMIN_TELEGRAM_USERNAME.lstrip('@')
                admin_msg = (
                    f"üÜò **–ê–ª–µ–∫—Å–µ–π –∑–æ–≤–µ—Ç –Ω–∞ –ø–æ–º–æ—â—å!**\n\n"
                    f"–ö–ª–∏–µ–Ω—Ç: {sender.first_name} (@{sender.username})\n"
                    f"–í–æ–ø—Ä–æ—Å: {question}\n\n"
                    f"–ë–æ–ª—å—à–µ —è –≤ —ç—Ç–æ—Ç –¥–∏–∞–ª–æ–≥ –Ω–µ –ø–∏—à—É. –¢–µ–ø–µ—Ä—å —Ç–≤–æ—è –æ—á–µ—Ä–µ–¥—å!"
                )
                try:
                    await client.send_message(admin_username, admin_msg)
                except: pass
                
                # Set handover flags
                lead.is_human_managed = True
                lead.handover_reason = f"AI requested help: {question}"
                
                # Clean the tag
                clean_response = re.sub(r"\[ASK_ADMIN:.*?\]", "", clean_response).strip()

            # Case 2: Graceful Exit (Colleague/Recruiter detected)
            if "[HANDOVER_TO_HUMAN:" in ai_response_text:
                match = re.search(r"\[HANDOVER_TO_HUMAN:\s*(.*?)\]", ai_response_text)
                reason = match.group(1) if match else "–¢–∞–∫—Ç–∏—á–Ω—ã–π –æ—Ç—Ö–æ–¥"
                
                # Notify Admin
                admin_username = settings.ADMIN_TELEGRAM_USERNAME.lstrip('@')
                admin_msg = (
                    f"ü§ù **–¢–∞–∫—Ç–∏—á–Ω—ã–π –æ—Ç—Ö–æ–¥ –ê–ª–µ–∫—Å–µ—è**\n\n"
                    f"–õ–∏–¥: {sender.first_name} (@{sender.username})\n"
                    f"–ü—Ä–∏—á–∏–Ω–∞: {reason}\n\n"
                    f"–ê–ª–µ–∫—Å–µ–π —Ä–∞—Å–ø–æ–∑–Ω–∞–ª –∫–æ–ª–ª–µ–≥—É –∏–ª–∏ —Ä–µ–∫—Ä—É—Ç–µ—Ä–∞ –∏ –≤—ã–¥–∞–ª —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Ñ—Ä–∞–∑—É. –î–∏–∞–ª–æ–≥ –ø–µ—Ä–µ–¥–∞–Ω —Ç–µ–±–µ (–ò–ò –æ—Ç–∫–ª—é—á–µ–Ω)."
                )
                try:
                    await client.send_message(admin_username, admin_msg)
                except: pass
                
                # Set handover flags
                lead.is_human_managed = True
                lead.handover_reason = f"Graceful exit: {reason}"
                
                # Clean the tag
                clean_response = re.sub(r"\[HANDOVER_TO_HUMAN:.*?\]", "", clean_response).strip()

            # Re-split chunks based on cleaned response
            chunks = humanity_manager.split_into_human_chunks(clean_response)

            # 7.4 Send chunks with typing simulation
            for i, chunk in enumerate(chunks):
                # Start typing simulation
                duration = humanity_manager.get_typing_duration(chunk)
                async with client.action(event.chat_id, 'typing'):
                    await asyncio.sleep(duration)
                    
                    # If it's the LAST chunk, we might want to attach cases
                    if i == len(chunks) - 1:
                        if mentioned_cases:
                            # Sort to get the most relevant first
                            best_case = mentioned_cases[0]
                            
                            if best_case.image_url and os.path.exists(best_case.image_url):
                                caption = (
                                    f"{chunk}\n\n"
                                    f"üìä –ö–µ–π—Å: {best_case.title}\n"
                                    f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: {best_case.results}\n\n"
                                    f"üîó –ß–∏—Ç–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π —Ä–∞–∑–±–æ—Ä: {best_case.project_url}"
                                )
                                if len(caption) > 1000:
                                    caption = caption[:997] + "..."
                                # Use client directly
                                sent_msg = await client.send_file(event.chat_id, best_case.image_url, caption=caption)
                            else:
                                text_with_link = f"{chunk}\n\nüîó –ü–æ–¥—Ä–æ–±–Ω–µ–µ –æ–± —ç—Ç–æ–º –∫–µ–π—Å–µ: {best_case.project_url}"
                                sent_msg = await client.send_message(event.chat_id, text_with_link)
                        else:
                            sent_msg = await client.send_message(event.chat_id, chunk)
                    else:
                        # Just send a regular message chunk
                        sent_msg = await client.send_message(event.chat_id, chunk)
                    
                    if sent_msg:
                        handover_manager.mark_as_automated(sent_msg.id)
                
                # Pause between chunks
                if i < len(chunks) - 1:
                    import random
                    pause = random.uniform(0.7, 1.8)
                    await asyncio.sleep(pause)
                    
        except Exception as e:
            logger.error(f"Error sending message with humanity: {e}")
            status = "failed"
        
        # 8. Log Outgoing
        out_msg_log = MessageLog(
            lead_id=lead.id, direction="outgoing", content=ai_response_text,
            status=status, telegram_msg_id=sent_msg.id if sent_msg else None
        )
        session.add(out_msg_log)
        lead.last_interaction = datetime.utcnow()
        await session.commit()

        
        # 9. Adaptive Learning
        try:
            new_history = history_text + f"\n–ö–ª–∏–µ–Ω—Ç: {full_text}\n–ê–ª–µ–∫—Å–µ–π: {ai_response_text}"
            analysis_prompt = prompt_builder.build_analysis_prompt(new_history, lead.style_profile or "", lead.context_memory or "")
            analysis_json = await llm_client.generate_response(analysis_prompt, "–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ —Å—Ç–∏–ª—è –æ–±—â–µ–Ω–∏—è.")
            start, end = analysis_json.find('{'), analysis_json.rfind('}') + 1
            if start != -1 and end != -1:
                data = json.loads(analysis_json[start:end])
                lead.style_profile = data.get("style_profile", lead.style_profile)
                lead.context_memory = data.get("context_memory", lead.context_memory)
                await session.commit()
        except Exception: pass

async def handle_message_read(event: MessageRead):
    if event.inbox: return
    async with async_session() as session:
        stmt = update(MessageLog).where(and_(MessageLog.telegram_msg_id <= event.max_id, MessageLog.direction == 'outgoing', MessageLog.status == 'sent')).values(status='read')
        await session.execute(stmt)
        await session.commit()
