import json
import os
import sqlite3
from typing import Dict, List
from datetime import datetime, timedelta
from core.ai_engine.llm_client import llm_client
from core.utils.logger import logger
from core.config.settings import settings

class GwenLearningEngine:
    """
    –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ì–≤–µ–Ω.
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Ñ–∏–ª—å—Ç—Ä—ã —á–µ—Ä–µ–∑ LLM.
    """
    
    def __init__(self):
        self.db_path = "vacancies.db"
        self.filters_path = os.path.join(settings.BASE_DIR, "core/config/dynamic_filters.json")

    async def run_learning_session(self) -> Dict:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç —Å–µ—Å—Å–∏—é –æ–±—É—á–µ–Ω–∏—è.
        """
        logger.info("üß† –ì–≤–µ–Ω –Ω–∞—á–∏–Ω–∞–µ—Ç —Å–µ—Å—Å–∏—é —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è...")
        
        # 1. –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î
        data = self._get_recent_data()
        if not data['accepted'] and not data['rejected']:
            return {"status": "skipped", "reason": "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"}
            
        import re
        import json
        
        # 2. –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ DeepSeek
        prompt = self._build_learning_prompt(data)
        
        # 3. –ó–∞–ø—Ä–æ—Å –∫ LLM —á–µ—Ä–µ–∑ OpenRouter
        system_prompt = (
            "–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö –∏ —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å–ø–∞–º–∞. "
            "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–ø–∏—Å–æ–∫ –æ–¥–æ–±—Ä–µ–Ω–Ω—ã—Ö –∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π "
            "–∏ –≤—ã–¥–µ–ª–∏—Ç—å –Ω–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–ª–∏ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏. "
            "–û–¢–í–ï–ß–ê–ô –°–¢–†–û–ì–û –í –§–û–†–ú–ê–¢–ï JSON. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –¥–≤–æ–π–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏. "
            "–§–æ—Ä–º–∞—Ç: {\"positive\": [], \"negative\": [], \"explanation\": \"\"}"
        )
        
        response_text = await llm_client.generate_response(prompt, system_prompt)
        if not response_text:
            return {"status": "error", "reason": "LLM –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ –æ—Ç–≤–µ—Ç"}
            
        # 4. –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ –æ–±–Ω–æ–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ (—É—á–∏—Ç—ã–≤–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ ```json –±–ª–æ–∫–∏)
            start = response_text.find('{')
            end = response_text.rfind('}') + 1
            if start == -1: 
                logger.error(f"–ì–≤–µ–Ω –Ω–µ –Ω–∞—à–ª–∞ JSON –≤ –æ—Ç–≤–µ—Ç–µ: {response_text}")
                raise ValueError("JSON not found")
            
            json_str = response_text[start:end]
            # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ (–∏–Ω–æ–≥–¥–∞ –º–æ–¥–µ–ª–∏ —Å—Ç–∞–≤—è—Ç –∑–∞–ø—è—Ç—ã–µ –ø–µ—Ä–µ–¥ })
            json_str = re.sub(r',\s*}', '}', json_str)
            
            new_rules = json.loads(json_str)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∞–π–ª
            updated_count = self._update_filters(new_rules)
            
            logger.info(f"‚úÖ –ì–≤–µ–Ω —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–∏–ª–∞ —Ñ–∏–ª—å—Ç—Ä—ã. –î–æ–±–∞–≤–ª–µ–Ω–æ: {updated_count} –ø—Ä–∞–≤–∏–ª.")
            return {
                "status": "success", 
                "added_positive": new_rules.get("positive", []),
                "added_negative": new_rules.get("negative", []),
                "reason": new_rules.get("explanation", "–ü–ª–∞–Ω–æ–≤–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤")
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –ø—Ä–∞–≤–∏–ª –æ—Ç LLM: {e}")
            return {"status": "error", "reason": str(e)}

    def _get_recent_data(self) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∑–∞ —Ç–µ–∫—É—â–∏–µ —Å—É—Ç–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # –ù–∞—á–∞–ª–æ —Å–µ–≥–æ–¥–Ω—è—à–Ω–µ–≥–æ –¥–Ω—è (00:00)
        today_start = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0).strftime('%Y-%m-%d %H:%M:%S')
        
        # –ë–µ—Ä–µ–º –≤—Å–µ –æ–¥–æ–±—Ä–µ–Ω–Ω—ã–µ –∑–∞ —Å–µ–≥–æ–¥–Ω—è
        cursor.execute("SELECT text FROM vacancies WHERE status = 'accepted' AND last_seen >= ?", (today_start,))
        accepted = [row['text'] for row in cursor.fetchall()]
        
        # –ë–µ—Ä–µ–º –≤—Å–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–Ω—ã–µ (–º—É—Å–æ—Ä) –∑–∞ —Å–µ–≥–æ–¥–Ω—è
        cursor.execute("SELECT text FROM vacancies WHERE status = 'rejected' AND last_seen >= ?", (today_start,))
        rejected = [row['text'] for row in cursor.fetchall()]
        
        # –ï—Å–ª–∏ –∑–∞ —Å–µ–≥–æ–¥–Ω—è –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —É—Ç—Ä–æ), –¥–æ–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20 —à—Ç—É–∫ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if len(accepted) < 5:
            cursor.execute("SELECT text FROM vacancies WHERE status = 'accepted' ORDER BY last_seen DESC LIMIT 20")
            accepted = list(set(accepted + [row['text'] for row in cursor.fetchall()]))
            
        if len(rejected) < 10:
            cursor.execute("SELECT text FROM vacancies WHERE status = 'rejected' ORDER BY last_seen DESC LIMIT 30")
            rejected = list(set(rejected + [row['text'] for row in cursor.fetchall()]))
        
        conn.close()
        return {"accepted": accepted, "rejected": rejected}

    def _build_learning_prompt(self, data: Dict) -> str:
        accepted_text = "\n---\n".join([t[:300] for t in data['accepted']])
        rejected_text = "\n---\n".join([t[:300] for t in data['rejected']])
        
        prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –∏ –≤—ã–¥–µ–ª–∏ –Ω–æ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–ª–∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã (—Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è), –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥—É—Ç –ª—É—á—à–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –º—É—Å–æ—Ä –∏ –Ω–∞—Ö–æ–¥–∏—Ç—å –ø–æ–ª–µ–∑–Ω–æ–µ.

–¶–ï–õ–¨: –ú—ã –∏—â–µ–º –∑–∞–¥–∞—á–∏ –ø–æ SEO, –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ä–µ–∫–ª–∞–º–µ (–î–∏—Ä–µ–∫—Ç), –ê–≤–∏—Ç–æ (—Ç–æ–ª—å–∫–æ –∞–≤–∏—Ç–æ–ª–æ–≥/–ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ), –†–∞–∑—Ä–∞–±–æ—Ç–∫–µ —Å–∞–π—Ç–æ–≤ (–¢–∏–ª—å–¥–∞) –∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–º—É –º–∞—Ä–∫–µ—Ç–∏–Ω–≥—É.
–ù–ï –ò–ù–¢–ï–†–ï–°–ù–û: SMM, Email, –î–∏–∑–∞–π–Ω, –í–∏–¥–µ–æ–º–æ–Ω—Ç–∞–∂, –ö–æ–ø–∏—Ä–∞–π—Ç–∏–Ω–≥ (–∫—Ä–æ–º–µ SEO), –ê–Ω–∞–ª–∏—Ç–∏–∫–∞, –ü—Ä–æ–¥–∞–∂–∏, –ü–æ–∏—Å–∫ –ø–µ—Ä—Å–æ–Ω–∞–ª–∞, –ò–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–∞, –∫–∞—Ä—Ç–æ—á–∫–∏ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤ (WB/Ozon), —á–∞—Ç-–±–æ—Ç—ã, Mini App, –ø—Ä–æ—Å—Ç–æ –ø—É–±–ª–∏–∫–∞—Ü–∏—è –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–ª–∏ –æ—Ç—Ä–∏—Å–æ–≤–∫–∞ –±–∞–Ω–Ω–µ—Ä–æ–≤ –¥–ª—è –ê–≤–∏—Ç–æ.

‚úÖ –ü–†–ò–ú–ï–†–´ –û–î–û–ë–†–ï–ù–ù–´–• (–•–æ—Ä–æ—à–∏–µ):
{accepted_text}

‚ùå –ü–†–ò–ú–ï–†–´ –û–¢–ö–õ–û–ù–ï–ù–ù–´–• (–ú—É—Å–æ—Ä):
{rejected_text}

–í—ã–¥–∞–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
  "positive": ["–Ω–æ–≤–æ–µ_—Å–ª–æ–≤–æ1", "—Ñ—Ä–∞–∑–∞2"],
  "negative": ["–º—É—Å–æ—Ä–Ω–æ–µ_—Å–ª–æ–≤–æ1", "—Ä–µ–≥—É–ª—è—Ä–∫–∞2"],
  "explanation": "–∫—Ä–∞—Ç–∫–æ–µ –ø–æ—è—Å–Ω–µ–Ω–∏–µ, –ø–æ—á–µ–º—É –¥–æ–±–∞–≤–ª–µ–Ω—ã —ç—Ç–∏ —Å–ª–æ–≤–∞"
}}

–í–∞–∂–Ω–æ: –ù–µ –¥—É–±–ª–∏—Ä—É–π —É–∂–µ –æ–±—â–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ (SEO, –î–∏—Ä–µ–∫—Ç –∏ —Ç.–¥.). –ò—â–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å–ø–∞–º–∞ –∏–ª–∏ –Ω–æ–≤—ã—Ö –Ω–∏—à.
"""
        return prompt

    async def expand_semantics(self) -> Dict:
        """
        –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç —É –ò–ò —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–¥—Ä–∞ (–∂–∏–≤—ã–µ —Ñ—Ä–∞–∑—ã –¥–ª—è –≤—Å–µ—Ö –Ω–∏—à).
        """
        logger.info("üì° –ì–≤–µ–Ω –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ —É DeepSeek...")
        
        prompt = """
–°–æ—Å—Ç–∞–≤—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —à–∏—Ä–æ–∫–∏–π —Å–ø–∏—Å–æ–∫ "–∂–∏–≤—ã—Ö" –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Ñ—Ä–∞–∑ –∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è Telegram-—á–∞—Ç–æ–≤ –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º:
1. SEO (–ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ —Å–∞–π—Ç–æ–≤)
2. –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è —Ä–µ–∫–ª–∞–º–∞ (–Ø–Ω–¥–µ–∫—Å –î–∏—Ä–µ–∫—Ç)
3. –ê–≤–∏—Ç–æ (–ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ, —É—Å–ª—É–≥–∏ –∞–≤–∏—Ç–æ–ª–æ–≥–∞)
4. –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Å–∞–π—Ç–æ–≤ (–¢–∏–ª—å–¥–∞, –ª–µ–Ω–¥–∏–Ω–≥–∏, —Å–∞–π—Ç—ã –ø–æ–¥ –∫–ª—é—á)
5. –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –º–∞—Ä–∫–µ—Ç–∏–Ω–≥ (—Ç—Ä–∞—Ñ–∏–∫, –ª–∏–¥—ã, —Ä–∞–∑–≤–∏—Ç–∏–µ –±–∏–∑–Ω–µ—Å–∞)

–§–û–†–ú–ê–¢: –ò—â–∏ —Ñ—Ä–∞–∑—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–∏—à—É—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –ª—é–¥–∏ (¬´–Ω—É–∂–µ–Ω —Å–∞–π—Ç¬ª, ¬´—Ö–æ—á—É –≤ —Ç–æ–ø¬ª, ¬´–∫—Ç–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç –¥–∏—Ä–µ–∫—Ç¬ª, ¬´–ø–æ–º–æ–≥–∏—Ç–µ —Å –ª–∏–¥–∞–º–∏¬ª).
–ò–°–ö–õ–Æ–ß–ò: SMM, –¥–∏–∑–∞–π–Ω, –∫–∞—Ä—Ç–æ—á–∫–∏ WB, —á–∞—Ç-–±–æ—Ç–æ–≤, –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ —à—Ç–∞—Ç, –ø–æ–∏—Å–∫ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤.

–í—ã–¥–∞–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{
  "positive": ["—Ñ—Ä–∞–∑–∞1", "—Ñ—Ä–∞–∑–∞2", ...],
  "count": 0,
  "explanation": "–ø–æ—á–µ–º—É –≤—ã–±—Ä–∞–Ω—ã —ç—Ç–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è"
}
–ü–∏—à–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –¥–≤–æ–π–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏. –ú–∏–Ω–∏–º—É–º 30 —Ñ—Ä–∞–∑.
"""
        system_prompt = "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –∏ –ø–æ–∏—Å–∫–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º –≤ –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä–∞—Ö."
        
        response_text = await llm_client.generate_response(prompt, system_prompt)
        if not response_text:
            return {"status": "error", "reason": "LLM –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ –æ—Ç–≤–µ—Ç"}

        logger.info(f"Raw LLM response (full): {response_text}")  # Debug log

        try:
            import re
            start = response_text.find('{')
            end = response_text.rfind('}') + 1
            
            if start == -1:
                logger.error(f"JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ: {response_text}")
                return {"status": "error", "reason": "JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ LLM"}
            
            json_str = response_text[start:end]
            # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞
            json_str = re.sub(r',\s*}', '}', json_str)
            json_str = re.sub(r',\s*]', ']', json_str)
            
            new_rules = json.loads(json_str)
            
            added = self._update_filters({"positive": new_rules.get("positive", []), "negative": []})
            
            return {
                "status": "success",
                "added_count": added,
                "phrases": new_rules.get("positive", []),
                "reason": "–ú–∞—Å—à—Ç–∞–±–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–¥—Ä–∞"
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏: {e}")
            return {"status": "error", "reason": str(e)}

    async def analyze_approval_reason(self, text: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –û–î–û–ë–†–ò–õ.
        –ü–æ–º–æ–≥–∞–µ—Ç –ì–≤–µ–Ω –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ –∏–º–µ–Ω–Ω–æ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞–º –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã.
        """
        prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –û–î–û–ë–†–ò–õ.
        –í—ã–¥–µ–ª–∏ –≥–ª–∞–≤–Ω—É—é –ø—Ä–∏—á–∏–Ω—É (–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ –Ω–∏—à—É), –ø–æ—á–µ–º—É —ç—Ç–æ –•–û–†–û–®–ò–ô –ª–∏–¥.
        –ù–∞—à–∏ –Ω–∏—à–∏: SEO, –î–∏—Ä–µ–∫—Ç, –ê–≤–∏—Ç–æ (–ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ), –°–∞–π—Ç—ã.

        –¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è:
        ---
        {text[:500]}
        ---

        –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ (–¥–æ 10 —Å–ª–æ–≤).
        """
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º Ollama
            reason = await llm_client._generate_ollama(settings.OLLAMA_MODEL, prompt, "–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ –ª–∏–¥–æ–≤.")
            if not reason or "error" in reason.lower() or "connection" in reason.lower():
                raise ConnectionError("Ollama is unavailable")
            
            # –ï—Å–ª–∏ Ollama —Å—Ä–∞–±–æ—Ç–∞–ª–∞, –∏–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            extraction_prompt = f"–ò–∑–≤–ª–µ–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤—ã—à–µ 1‚Äì2 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞/—Ñ—Ä–∞–∑—ã, –∫–æ—Ç–æ—Ä—ã–µ –¥–µ–ª–∞—é—Ç –µ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º. –û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞–º–∏ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é.\n–¢–µ–∫—Å—Ç: {text[:200]}"
            keywords = await llm_client._generate_ollama(settings.OLLAMA_MODEL, extraction_prompt, "–¢—ã ‚Äî —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.")
            
            if keywords and len(keywords) < 50:
                new_kw = [k.strip().lower() for k in keywords.split(',') if len(k.strip()) > 3]
                if new_kw:
                    self._update_filters({"positive": new_kw, "negative": []})
                    logger.info(f"‚ú® –ì–≤–µ–Ω –≤—ã—É—á–∏–ª–∞ –Ω–æ–≤—ã–µ –ü–†–ò–û–†–ò–¢–ï–¢–ù–´–ï —Å–ª–æ–≤–∞: {new_kw}")
            
            return reason or "–û–¥–æ–±—Ä–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º"

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ollama fail in approval analysis: {e}. Switching to Cloud...")
            # Fallback –Ω–∞ OpenRouter
            try:
                reason = await llm_client.generate_response(prompt, system_prompt="–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ –ª–∏–¥–æ–≤. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ.")
                return reason or "–û–¥–æ–±—Ä–µ–Ω–æ (Cloud)"
            except:
                return "–û–¥–æ–±—Ä–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º"

    async def analyze_spam_with_feedback(self, text: str, user_reason: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–ø–∞–º, –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        –ò—â–µ—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø—Ä–∏—á–∏–Ω—ã –≤ —Ç–µ–∫—Å—Ç–µ –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Ñ–∏–ª—å—Ç—Ä—ã.
        """
        prompt = f"""
        –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ—Ç–∫–ª–æ–Ω–∏–ª —ç—Ç–æ—Ç –ª–∏–¥ —Å –ø—Ä–∏—á–∏–Ω–æ–π: "{user_reason}".
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ –∏ –Ω–∞–π–¥–∏ –≤ –Ω–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–ª–∏ —Ñ—Ä–∞–∑—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —ç—Ç—É –ø—Ä–∏—á–∏–Ω—É.
        –ò–∑–≤–ª–µ–∫–∏ 1-2 —Å–∞–º—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã—Ö —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞.

        –¢–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏:
        ---
        {text[:700]}
        ---

        –û—Ç–≤–µ—Ç–∏—à—å —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–∫–æ–º —Å–ª–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é. –ï—Å–ª–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –Ω–µ—Ç, –æ—Ç–≤–µ—Ç—å "None".
        """
        try:
            # –ü—Ä–æ–±—É–µ–º Ollama —Å –∫–æ—Ä–æ—Ç–∫–∏–º —Ç–∞–π–º–∞—É—Ç–æ–º
            keywords = None
            try:
                keywords = await asyncio.wait_for(
                    llm_client._generate_ollama(settings.OLLAMA_MODEL, prompt, "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å–ø–∞–º–∞."),
                    timeout=5.0
                )
            except:
                logger.warning("Ollama timeout/fail in feedback analysis, switching to Cloud.")
            
            # Fallback –Ω–∞ Cloud –µ—Å–ª–∏ Ollama —Ç—É–ø–∏—Ç
            if not keywords or "error" in keywords.lower() or "connection" in keywords.lower():
                keywords = await llm_client.generate_response(prompt, system_prompt="–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–ø–∞–º—É. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–∫–æ–º –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.")

            if keywords and "none" not in keywords.lower() and len(keywords) < 100:
                new_kw = [k.strip().lower() for k in keywords.split(',') if len(k.strip()) > 3]
                if new_kw:
                    self._update_filters({"positive": [], "negative": new_kw})
                    logger.info(f"üíæ –ì–≤–µ–Ω –∑–∞–ø–æ–º–Ω–∏–ª–∞ –Ω–æ–≤—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–≤–æ–µ–≥–æ —Ñ–∏–¥–±–µ–∫–∞: {new_kw}")
                    return f"–ü–æ–Ω—è–ª–∞. –í—ã–¥–µ–ª–∏–ª–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞: {', '.join(new_kw)}"
            
            return "–ü—Ä–∏–Ω—è—Ç–æ. –Ø –∑–∞–ø–æ–º–Ω–∏–ª–∞ —ç—Ç—É –ø—Ä–∏—á–∏–Ω—É."
        except Exception as e:
            logger.error(f"Feedback analysis error: {e}")
            return "–ó–∞–ø–æ–º–Ω–∏–ª–∞."

    async def analyze_spam_reason(self, text: str) -> str:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–∏—á–∏–Ω—É —Å–ø–∞–º–∞ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)."""
        prompt = f"–ü–æ—á–µ–º—É —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç - —Å–ø–∞–º –¥–ª—è digital-–∞–≥–µ–Ω—Ç—Å—Ç–≤–∞? –û—Ç–≤–µ—Ç—å 1 —Ñ—Ä–∞–∑–æ–π.\n–¢–µ–∫—Å—Ç: {text[:300]}"
        try:
             return await llm_client.generate_response(prompt, system_prompt="–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ —Å–ø–∞–º–∞.")
        except:
             return "–ü–æ–º–µ—á–µ–Ω–æ –∫–∞–∫ —Å–ø–∞–º"

    def _update_filters(self, new_rules: Dict) -> int:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Ñ–∞–π–ª –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤."""
        if not os.path.exists(self.filters_path):
            current = {"positive": [], "negative": [], "version": 1}
        else:
            with open(self.filters_path, 'r', encoding='utf-8') as f:
                current = json.load(f)
        
        added = 0
        for key in ["positive", "negative"]:
            current_vals = set(current.get(key, []))
            for val in new_rules.get(key, []):
                if val.lower() not in current_vals:
                    current[key].append(val.lower())
                    added += 1
        
        current["last_updated"] = datetime.now().isoformat()
        
        with open(self.filters_path, 'w', encoding='utf-8') as f:
            json.dump(current, f, ensure_ascii=False, indent=4)
            
        return added

    async def revalidate_pending_leads(self) -> int:
        """
        –ü—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ –≤—Å–µ–º 'accepted' –ª–∏–¥–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –µ—â–µ –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã (–Ω–µ—Ç response),
        –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Ö –∑–∞–Ω–æ–≤–æ —Å —É—á–µ—Ç–æ–º –ù–û–í–´–• —Ñ–∏–ª—å—Ç—Ä–æ–≤ (—Å—Ç–æ–ø-—Å–ª–æ–≤).
        –ï—Å–ª–∏ –Ω–∞—Ö–æ–¥–∏—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –Ω–µ–≥–∞—Ç–∏–≤–æ–º -> –º–µ–Ω—è–µ—Ç —Å—Ç–∞—Ç—É—Å –Ω–∞ rejected.
        
        Returns:
            int: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç—Å–µ—è–Ω–Ω—ã—Ö –ª–∏–¥–æ–≤.
        """
        logger.info("üßπ –ì–≤–µ–Ω –ø—Ä–æ–≤–æ–¥–∏—Ç —Ä–µ–≤–∞–ª–∏–¥–∞—Ü–∏—é –æ—á–µ—Ä–µ–¥–∏ –ª–∏–¥–æ–≤...")
        try:
            # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã
            if not os.path.exists(self.filters_path):
                return 0
            
            with open(self.filters_path, 'r', encoding='utf-8') as f:
                filters = json.load(f)
                negative_keywords = filters.get("negative", [])

            if not negative_keywords:
                return 0

            # 2. –ë–µ—Ä–µ–º –≤—Å–µ 'accepted' –ª–∏–¥—ã –±–µ–∑ –æ—Ç–≤–µ—Ç–∞
            conn = sqlite3.connect(self.db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            cursor.execute("SELECT hash, text FROM vacancies WHERE status = 'accepted' AND (response IS NULL OR response = '')")
            pending_leads = cursor.fetchall()
            
            count_rejected = 0
            
            for lead in pending_leads:
                text_lower = lead['text'].lower()
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ–≥–∞—Ç–∏–≤
                found_negative = None
                for neg in negative_keywords:
                    if neg in text_lower:
                        found_negative = neg
                        break
                
                if found_negative:
                    # –ù–∞—à–ª–∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–æ! –û—Ç–∫–ª–æ–Ω—è–µ–º.
                    logger.info(f"üßπ –†–µ–≤–∞–ª–∏–¥–∞—Ü–∏—è: –æ—Ç–∫–ª–æ–Ω–µ–Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—è {lead['hash']} (—Å—Ç–æ–ø-—Å–ª–æ–≤–æ: {found_negative})")
                    cursor.execute(
                        "UPDATE vacancies SET status = 'rejected', rejection_reason = ? WHERE hash = ?", 
                        (f"AUTO_REVALIDATION: {found_negative}", lead['hash'])
                    )
                    count_rejected += 1
            
            conn.commit()
            conn.close()
            
            if count_rejected > 0:
                logger.info(f"‚úÖ –†–µ–≤–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û—á–∏—â–µ–Ω–æ –ª–∏–¥–æ–≤: {count_rejected}")
            
            return count_rejected
            
        except Exception as e:
            logger.error(f"Error during revalidation: {e}")
            return 0

# Singleton
gwen_learning_engine = GwenLearningEngine()
