#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ semantic deduplication –Ω–∞ edge cases.
"""

import sys
import asyncio
import os

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç–∏ –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, os.getcwd())

from systems.parser.duplicate_detector import get_duplicate_detector
from core.utils.structured_logger import get_logger

logger = get_logger(__name__)


def test_semantic_similarity():
    """–¢–µ—Å—Ç semantic similarity calculation"""
    print("=== –¢–µ—Å—Ç 1: Semantic Similarity ===\n")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±–µ–∑ DB –¥–ª—è —á–∏—Å—Ç—ã—Ö unit —Ç–µ—Å—Ç–æ–≤
    detector = get_duplicate_detector()
    
    if not detector.semantic_enabled:
        print("‚ùå Semantic deduplication –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞")
        return
    
    test_pairs = [
        # Pair 1: –Ø–≤–Ω—ã–µ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∫–∏ (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥—É–±–ª—è–º–∏)
        (
            "–ò—â—É SEO-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –¥–ª—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è —Å–∞–π—Ç–∞. –ë—é–¥–∂–µ—Ç 50000‚ÇΩ.",
            "–ù—É–∂–µ–Ω —Å–µ–æ—à–Ω–∏–∫, —á—Ç–æ–±—ã –≤—ã–≤–µ—Å—Ç–∏ —Ä–µ—Å—É—Ä—Å –≤ —Ç–æ–ø. –î–æ 50–∫ –≥–æ—Ç–æ–≤ –ø–ª–∞—Ç–∏—Ç—å.",
            True  # Expected duplicate
        ),
        
        # Pair 2: –†–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏
        (
            "–¢—Ä–µ–±—É–µ—Ç—Å—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω–∞ –Ω–∞ WordPress.",
            "–ò—â—É –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–∞–π—Ç–∞-–º–∞–≥–∞–∑–∏–Ω–∞. CMS - –í–æ—Ä–¥–ü—Ä–µ—Å—Å.",
            True
        ),
        
        # Pair 3: –ü–æ—Ö–æ–∂–∞—è —Ç–µ–º–∞, –Ω–æ —Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏ (–ù–ï –¥—É–±–ª–∏)
        (
            "–ù—É–∂–µ–Ω SEO-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –¥–ª—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω–∞ –∫–æ—Å–º–µ—Ç–∏–∫–∏.",
            "–ò—â—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–ª–æ–≥–∞ –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ø–Ω–¥–µ–∫—Å.–î–∏—Ä–µ–∫—Ç –¥–ª—è –∫–æ—Å–º–µ—Ç–∏–∫–∏.",
            False
        ),
        
        # Pair 4: –¢–æ—á–Ω–∞—è –∫–æ–ø–∏—è (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥—É–±–ª—å)
        (
            "–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –ª–µ–Ω–¥–∏–Ω–≥–∞. –ë—é–¥–∂–µ—Ç –¥–æ 100 —Ç—ã—Å—è—á —Ä—É–±–ª–µ–π.",
            "–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –ª–µ–Ω–¥–∏–Ω–≥–∞. –ë—é–¥–∂–µ—Ç –¥–æ 100 —Ç—ã—Å—è—á —Ä—É–±–ª–µ–π.",
            True
        ),
        
        # Pair 5: –°–æ–≤–µ—Ä—à–µ–Ω–Ω–æ —Ä–∞–∑–Ω—ã–µ —Ç–µ–º—ã (–ù–ï –¥—É–±–ª–∏)
        (
            "–ù—É–∂–µ–Ω SEO-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –¥–ª—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è —Å–∞–π—Ç–∞.",
            "–ò—â—É –¥–∏–∑–∞–π–Ω–µ—Ä–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ª–æ–≥–æ—Ç–∏–ø–∞.",
            False
        ),
    ]
    
    results = []
    
    for i, (text1, text2, expected_dup) in enumerate(test_pairs, 1):
        semantic_sim = detector.calculate_semantic_similarity(text1, text2)
        exact_sim = detector.calculate_exact_similarity(text1, text2)
        
        is_duplicate = semantic_sim > detector.semantic_threshold
        
        status = "‚úÖ" if is_duplicate == expected_dup else "‚ùå"
        
        print(f"{status} Pair {i}:")
        print(f"   Text 1: {text1[:60]}...")
        print(f"   Text 2: {text2[:60]}...")
        print(f"   Semantic: {semantic_sim:.3f} (threshold: {detector.semantic_threshold})")
        print(f"   Exact: {exact_sim:.3f}")
        print(f"   Result: {'DUPLICATE' if is_duplicate else 'UNIQUE'}")
        print(f"   Expected: {'DUPLICATE' if expected_dup else 'UNIQUE'}")
        print()
        
        results.append(is_duplicate == expected_dup)
    
    accuracy = sum(results) / len(results) * 100
    print(f"Accuracy: {accuracy:.1f}% ({sum(results)}/{len(results)})")


def test_exact_vs_semantic():
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ exact match vs semantic similarity"""
    print("\n=== –¢–µ—Å—Ç 2: Exact vs Semantic ===\n")
    
    detector = get_duplicate_detector()
    
    if not detector.semantic_enabled:
        print("‚ùå Semantic deduplication –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞")
        return
    
    # –ö–µ–π—Å—ã –≥–¥–µ semantic –ª—É—á—à–µ exact match
    edge_cases = [
        (
            "–ò—â—É SEO-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –¥–ª—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è —Å–∞–π—Ç–∞ –∫–ª–∏–Ω–∏–∫–∏",
            "–ù—É–∂–µ–Ω —Å–µ–æ—à–Ω–∏–∫ –¥–ª—è —Ä–∞—Å–∫—Ä—É—Ç–∫–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ —Å–∞–π—Ç–∞"
        ),
        (
            "–¢—Ä–µ–±—É–µ—Ç—Å—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Å–∞–π—Ç–∞ –Ω–∞ React",
            "–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –≤–µ–±-—Ä–µ—Å—É—Ä—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ReactJS"
        ),
        (
            "–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ê–≤–∏—Ç–æ –º–∞–≥–∞–∑–∏–Ω–∞",
            "–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Avito –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤"
        ),
    ]
    
    print("–ö–µ–π—Å—ã –≥–¥–µ SEMANTIC > EXACT:\n")
    
    for i, (text1, text2) in enumerate(edge_cases, 1):
        semantic_sim = detector.calculate_semantic_similarity(text1, text2)
        exact_sim = detector.calculate_exact_similarity(text1, text2)
        
        improvement = semantic_sim - exact_sim
        status = "‚úÖ" if improvement > 0.2 else "‚ö†Ô∏è"
        
        print(f"{status} Case {i}:")
        print(f"   Text 1: {text1}")
        print(f"   Text 2: {text2}")
        print(f"   Semantic: {semantic_sim:.3f}")
        print(f"   Exact: {exact_sim:.3f}")
        print(f"   Improvement: +{improvement:.3f}")
        print()


def test_threshold_tuning():
    """–¢–µ—Å—Ç —Ä–∞–∑–ª–∏—á–Ω—ã—Ö threshold values"""
    print("\n=== –¢–µ—Å—Ç 3: Threshold Tuning ===\n")
    
    detector = get_duplicate_detector()
    
    if not detector.semantic_enabled:
        print("‚ùå Semantic deduplication –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞")
        return
    
    # –ù–∞–±–æ—Ä –ø—Ä–∏–º–µ—Ä–æ–≤ —Å —Ä–∞–∑–Ω–æ–π —Å—Ç–µ–ø–µ–Ω—å—é similarity
    test_cases = [
        ("–ù—É–∂–µ–Ω SEO. –ë—é–¥–∂–µ—Ç 50–∫.", "–ò—â—É —Å–µ–æ—à–Ω–∏–∫–∞. –î–æ 50 —Ç—ã—Å—è—á.", "close_paraphrase"),
        ("–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Å–∞–π—Ç–∞ –Ω–∞ React", "–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è ReactJS", "paraphrase"),
        ("SEO –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ —Å–∞–π—Ç–∞", "–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è —Ä–µ–∫–ª–∞–º–∞ –Ø–Ω–¥–µ–∫—Å.–î–∏—Ä–µ–∫—Ç", "related_topic"),
        ("–ù—É–∂–µ–Ω –¥–∏–∑–∞–π–Ω–µ—Ä", "–ò—â—É –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–∞ Python", "different_topic"),
    ]
    
    thresholds = [0.70, 0.75, 0.80, 0.85]
    
    print("Similarity scores –∏ classifications:\n")
    
    for text1, text2, category in test_cases:
        sim = detector.calculate_semantic_similarity(text1, text2)
        
        print(f"Category: {category}")
        print(f"  Text 1: {text1}")
        print(f"  Text 2: {text2}")
        print(f"  Similarity: {sim:.3f}")
        print(f"  Classified as duplicate at thresholds:")
        
        for threshold in thresholds:
            is_dup = sim > threshold
            print(f"    {threshold}: {'‚úÖ YES' if is_dup else '‚ùå NO'}")
        print()
    
    print(f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π threshold: 0.75 (—Ç–µ–∫—É—â–∏–π: {detector.semantic_threshold})")


async def test_integration():
    """–¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –ë–î"""
    print("\n=== –¢–µ—Å—Ç 4: Database Integration ===\n")
    
    from systems.parser.vacancy_db import VacancyDatabase
    
    db = VacancyDatabase()
    await db.init_db()
    detector = get_duplicate_detector(db_manager=db)
    
    if not detector.semantic_enabled:
        print("‚ùå Semantic deduplication –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞")
        return
    
    # –ò–º–∏—Ç–∏—Ä—É–µ–º –≤—Ö–æ–¥—è—â–∏–π –ª–∏–¥
    test_lead = "–ù—É–∂–µ–Ω SEO-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –¥–ª—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è —Å–∞–π—Ç–∞ –∫–ª–∏–Ω–∏–∫–∏. –ë—é–¥–∂–µ—Ç –¥–æ 100–∫."
    
    print("–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–∞ –¥–ª—è:")
    print(f"  {test_lead}\n")
    
    is_dup, similarity, method = await detector.is_duplicate(
        text=test_lead,
        message_id=999999,
        source_channel="test_channel"
    )
    
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç:")
    print(f"  Is duplicate: {is_dup}")
    print(f"  Similarity: {similarity:.3f}")
    print(f"  Method: {method}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = detector.get_statistics()
    print(f"\nüìä Detector stats:")
    for key, value in stats.items():
        print(f"  {key}: {value}")


if __name__ == "__main__":
    try:
        test_semantic_similarity()
        test_exact_vs_semantic()
        test_threshold_tuning()
        
        print("\n" + "="*50)
        asyncio.run(test_integration())
        
        print("\n‚úÖ –í–°–ï –¢–ï–°–¢–´ –ó–ê–í–ï–†–®–ï–ù–´")
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
