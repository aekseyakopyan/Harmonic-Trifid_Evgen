#!/usr/bin/env python3
"""
Migration script –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è embeddings –¥–ª—è existing leads.
–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –ø–æ—Å–ª–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è semantic deduplication.
"""

import sys
import asyncio
import os
from datetime import datetime, timedelta

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç–∏ –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, os.getcwd())

from systems.parser.duplicate_detector import get_duplicate_detector
from systems.parser.vacancy_db import VacancyDatabase
from core.utils.structured_logger import get_logger

logger = get_logger(__name__)


async def migrate_embeddings():
    """
    –í—ã—á–∏—Å–ª–∏—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å embeddings –¥–ª—è –≤—Å–µ—Ö leads –±–µ–∑ embeddings.
    """
    print("=== Embedding Migration Script ===\n")
    
    # Initialize
    db = VacancyDatabase()
    detector = get_duplicate_detector(db_manager=db)
    
    if not detector.semantic_enabled:
        print("‚ùå Semantic deduplication –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞")
        print("   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É sentence-transformers")
        return
    
    print("‚úÖ Semantic deduplication –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    print(f"   Model: cointegrated/rubert-tiny")
    print(f"   Semantic threshold: {detector.semantic_threshold}")
    print()
    
    # –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ leads –±–µ–∑ embeddings
    print("1. –ó–∞–≥—Ä—É–∂–∞—é leads –±–µ–∑ embeddings...")
    
    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å leads –±–µ–∑ embeddings –Ω–∞–ø—Ä—è–º—É—é
    leads_without_embeddings = await asyncio.to_thread(db.get_leads_without_embeddings, 10000)
    
    total_count = len(leads_without_embeddings)
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {total_count} leads –±–µ–∑ embeddings")
    
    if total_count == 0:
        print("\n‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
        return
    
    # Batch processing
    print(f"\n2. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ embeddings (batch size: 32)...")
    
    processed = await detector.precompute_embeddings_batch(
        leads=leads_without_embeddings,
        batch_size=32
    )
    
    print(f"\n‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}/{total_count}")
    if total_count > 0:
        print(f"   Success rate: {processed/total_count*100:.1f}%")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = detector.get_statistics()
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞:")
    print(f"   Cache size: {stats['cache_size']}/{stats['cache_max_size']}")
    print(f"   Time window: {stats['time_window_hours']}h")


if __name__ == "__main__":
    try:
        asyncio.run(migrate_embeddings())
    except KeyboardInterrupt:
        print("\n\n‚ùå –ú–∏–≥—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
